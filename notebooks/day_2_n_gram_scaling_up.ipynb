{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd532aad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Here's our N-gram model: what we have so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66071bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "913342c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"By this liberty they entered into a very laudable emulation to do all of them \\\n",
    "what they saw did please one. If any of the gallants or ladies should say, Let us drink, \\\n",
    "they would all drink.  If any one of them said, Let us play, they all played.  If one said, \\\n",
    "Let us go a-walking into the fields they went all.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b3c09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ngrams for a corpus\n",
    "def ngrams(text, n):\n",
    "    n_grams = []\n",
    "    for i in range(n-1, len(tokenized_corpus)): \n",
    "        n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))\n",
    "    return n_grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0792006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "        \n",
    "    def _tokenize(self, corpus):\n",
    "        \n",
    "        tokenized_corpus = []\n",
    "        \n",
    "        # separate punctuation from previous word\n",
    "        spaced_corpus = re.sub(r'(\\w)([.,?!;:])', r'\\1 \\2', corpus) \n",
    "        \n",
    "        # split into sentences\n",
    "        sentences = spaced_corpus.split('.')\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokenized_corpus += words\n",
    "        \n",
    "        return tokenized_corpus\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012be51",
   "metadata": {},
   "source": [
    "# Scaling up\n",
    "\n",
    "If we keep increasing n, our generated text starts to repeat our input text almost word for word. To get interesting behavior, we have to increase the size of the corpus. Let's try with a much bigger corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6702e4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "!python3 -m nltk.downloader gutenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f955ed1",
   "metadata": {},
   "source": [
    "NLTK comes with several built in corpora, including a selection of books from project gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3c38e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import corpus using an alias to avoid namespace confustion with our corpus variable\n",
    "from nltk import corpus as corpiss \n",
    "\n",
    "corpiss.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e69afbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Poems',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Blake',\n",
       " '1789',\n",
       " ']',\n",
       " 'SONGS',\n",
       " 'OF',\n",
       " 'INNOCENCE',\n",
       " 'AND',\n",
       " 'OF',\n",
       " 'EXPERIENCE',\n",
       " 'and',\n",
       " 'THE',\n",
       " 'BOOK',\n",
       " 'of',\n",
       " 'THEL',\n",
       " 'SONGS',\n",
       " 'OF',\n",
       " 'INNOCENCE',\n",
       " 'INTRODUCTION',\n",
       " 'Piping',\n",
       " 'down',\n",
       " 'the',\n",
       " 'valleys',\n",
       " 'wild',\n",
       " ',',\n",
       " 'Piping',\n",
       " 'songs',\n",
       " 'of',\n",
       " 'pleasant',\n",
       " 'glee',\n",
       " ',',\n",
       " 'On',\n",
       " 'a',\n",
       " 'cloud',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'a']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kjv = corpiss.gutenberg.words('blake-poems.txt')\n",
    "list(kjv)[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296cfdb2",
   "metadata": {},
   "source": [
    "### A Few final adjustments\n",
    "\n",
    "We have some housekeeping things to take care of. \n",
    "\n",
    "1. Because we have encoded sentence breaks as a string of start and stop sequences, we now will generate a lot of them in our output. We add function to strip them out, and update our generate method to strip out these tokens before printing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "703885bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'take mark , and see now , and humble ye them , and seethe his flesh in running water , and be slain , and they rode upon the camels , and have washed their robes , and made unto themselves of the holy angels , and to solomon his son . even so , father ; for the press is full , the fats overflow ; for their wickedness . 119 : 59 i thought on my ways , as a seal upon him , and would none of my words . 30 : 37 and jacob took him rods of green poplar , and of beast : it is most holy unto him of his labour the days of jehoahaz . 97 : 3 a man shall dig a pit , and sold joseph to the ishmeelites for twenty pieces of silver out of the sheath thereof , and add unto it the fifth part unto pharaoh , and say to hezekiah , thus saith god the lord , after this manner therefore pray ye : our father which art in heaven , now is the judgment of moab . 134 : 3 the aged women likewise , that they escaped all safe to land . spots they are and blemishes , sporting themselves with their own works , as god liveth , who hath begotten me these , seeing i have rejected him from reigning over israel ? 15 : 37 and reuben spake unto his brother a name in israel , telleth the king of lachish , bind the tire of thine head upon thee , until it was a river that i could withstand god ? 11 : 30 are they not written in this book : worship god : for man would swallow me up . unto him that giveth his neighbour drink , that puttest thy bottle to him , rise , and measure the temple of babylon , my servant deceived me : my moisture is turned into mourning . 106 : 18 and samuel told him every whit , and hid snares for my feet .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_stops(string):\n",
    "    \"\"\"\n",
    "    function to convert the stop/start sequence back into periods.\n",
    "    strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "    and replaces them with a period.\n",
    "\n",
    "    then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "    string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "    string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "test_string = '<s> <s> <s> take mark , and see now , and humble ye them , and seethe his flesh in running water , and be slain , and they rode upon the camels , and have washed their robes , and made unto themselves of the holy angels , and to solomon his son </s> </s> </s> <s> <s> <s> even so , father ; for the press is full , the fats overflow ; for their wickedness </s> </s> </s> <s> <s> <s> 119 : 59 i thought on my ways , as a seal upon him , and would none of my words </s> </s> </s> <s> <s> <s> 30 : 37 and jacob took him rods of green poplar , and of beast : it is most holy unto him of his labour the days of jehoahaz </s> </s> </s> <s> <s> <s> 97 : 3 a man shall dig a pit , and sold joseph to the ishmeelites for twenty pieces of silver out of the sheath thereof , and add unto it the fifth part unto pharaoh , and say to hezekiah , thus saith god the lord , after this manner therefore pray ye : our father which art in heaven , now is the judgment of moab </s> </s> </s> <s> <s> <s> 134 : 3 the aged women likewise , that they escaped all safe to land </s> </s> </s> <s> <s> <s> spots they are and blemishes , sporting themselves with their own works , as god liveth , who hath begotten me these , seeing i have rejected him from reigning over israel ? 15 : 37 and reuben spake unto his brother a name in israel , telleth the king of lachish , bind the tire of thine head upon thee , until it was a river that i could withstand god ? 11 : 30 are they not written in this book : worship god : for man would swallow me up </s> </s> </s> <s> <s> <s> unto him that giveth his neighbour drink , that puttest thy bottle to him , rise , and measure the temple of babylon , my servant deceived me : my moisture is turned into mourning </s> </s> </s> <s> <s> <s> 106 : 18 and samuel told him every whit , and hid snares for my feet </s> </s> </s> <s> <s> <s>'\n",
    "model = NgramModel(corpus, 3)\n",
    "add_stops(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b86ba",
   "metadata": {},
   "source": [
    "2. numbers are a problem for n-gram models becayse there are so many of them. we don't want to eliminate them, because they are meaningful, but we want to abstract away from the individual numbers. In addition, we might want to get rid of some other things like parentheticals and quotes, becayse these impossible for our model to keep track of given it's amount of memory. We can take care of these things in the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f57c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def _tokenize(self, corpus):\n",
    "    # The list of regular expressions and replacements to be applied\n",
    "    # the order here matters! these replacements will happen in order\n",
    "    replacements = [\n",
    "         [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "        ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "        ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "        ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "        ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "    ]\n",
    "\n",
    "    # This is a function that applies a single replacement from the list\n",
    "    resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "\n",
    "    # we use the resub function to applea each replacement to the entire corpus,\n",
    "    normalized_corpus = reduce(resub, replacements, corpus)\n",
    "\n",
    "\n",
    "    sentences = normalized_corpus.split('.')\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split() # split on whitespace\n",
    "        words = [word.lower() for word in words]\n",
    "        words = list(pad_both_ends(words, n=self.n))\n",
    "        tokens += words\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b11df",
   "metadata": {},
   "source": [
    "Here is a final version of our class with all the bells and whistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e46957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from functools import reduce\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "\n",
    "    def _tokenize(self, corpus):\n",
    "        # The list of regular expressions and replacements to be applied\n",
    "        # the order here matters! these replacements will happen in order\n",
    "        replacements = [\n",
    "             [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "            ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "            ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "            ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "            ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "        ]\n",
    "        \n",
    "        # This is a function that applies a single replacement from the list\n",
    "        resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "        \n",
    "        # we use the resub function to applea each replacement to the entire corpus,\n",
    "        normalized_corpus = reduce(resub, replacements, corpus)\n",
    "        \n",
    "        \n",
    "        sentences = normalized_corpus.split('.')\n",
    "        \n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokens += words\n",
    "        \n",
    "        return tokens\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "        string = add_stops(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "    \n",
    "    def add_stops(string):\n",
    "        \"\"\"\n",
    "        function to convert the stop/start sequence back into periods.\n",
    "        strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "        and replaces them with a period.\n",
    "\n",
    "        then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "        string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "        string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "966c2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(corpus,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6c62143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by this liberty they saw did please one said , they went all . by this liberty they went all of them said , they went all drink , they saw did please one said , let us drink . if any of the gallants or ladies should say , they saw did please one said , they would all drink , let us drink . . if one of the gallants or ladies should say , let us go a walking into the fields they would all of them what they entered into the fields they went all of them what they went all . . if one said , let us play , they entered into the fields they saw did please one . . . if any one of them said , they would all of the gallants or ladies should say , they entered into the gallants or ladies should say , they saw did please one '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591a9ce",
   "metadata": {},
   "source": [
    "We try generating a 4-gram model with the King James Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b20358",
   "metadata": {},
   "source": [
    "Our model expects its training corpus in the form of a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4ee616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kjv = (' ').join(kjv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1be7089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(kjv, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2637c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thy wine doth purify the golden honey ; thy perfume . pale through pathless ways the fancied image strays , famished , weeping , weak , with hollow piteous shriek . so sang a little clod of clay , trodden with the cattle ' s feet , but a pebble of the brook warbled out these metres meet : love seeketh only self to please , to bind another to its delight , joys in another ' s loss of ease , and builds a hell in heaven ' s high bower , with silent delight , sits and smiles on the night . no , no , let us play , for it is yet day , and we cannot go to sleep ; besides , in the sky the little birds fly , and the hills are all covered with sheep . seven nights they sleep among shadows deep , and dream they see their child starved in desert wild    \""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1d6e2",
   "metadata": {},
   "source": [
    "# Let's do a mashup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c1a90",
   "metadata": {},
   "source": [
    "Intro to beautiful soup for scraping web text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86054ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#cover t-i-the-invisible-committe-now-8.png\n",
      "#pubdate 2018-02-25 19:37:42 +0000\n",
      "#title Now\n",
      "#author The Invisible Committe\n",
      "#LISTtitle Now\n",
      "#SORTauthors comité invisible\n",
      "#date 2017\n",
      "#source Retrived on February 18, 2018 from https://illwilleditions.noblogs.org/files/2018/02/Invisible-Committee-NOW-READ.pdf\n",
      "#lang en\n",
      "#SORTtopics insurrectionary, communization\n",
      "#notes The Invisible Committee are an anonymous fragment of the Imaginary Party.\n",
      "First published as *Maintenant* in May, 2017.\n",
      "Translated by Robert Hurley.\n",
      "\n",
      "\n",
      "No more waiting.\n",
      "No more hoping.\n",
      "No more letting ourselves be distracted, unnerved.\n",
      "Break and enter.\n",
      "Put untruth back in its place.\n",
      "Believe in what we feel.\n",
      "Act accordingly.\n",
      "Force our way into the present.\n",
      "Try. Fail this time. Try again. Fail better.\n",
      "Persist. Attack. Build.\n",
      "Go down one’s road.\n",
      "Win perhaps.\n",
      "In any case, overcome.\n",
      "Live, therefore.\n",
      "Now...\n",
      "\n",
      "\n",
      "** Tomorrow Is Cancelled\n",
      "\n",
      "\n",
      "\n",
      "[[t-i-the-invisible-committe-now-1.png f]]\n",
      "\n",
      "\n",
      "\n",
      "All the reasons for making a revolution are there. Not one is lacking. The shipwreck of politics, the arrogance of the powerful, the reign of falsehood, the vulgarity of the wealthy, the cataclysms of industry, galloping misery, naked exploitation, ecological apocalypse—we are spared nothing, not even being informed about it all. “Climate: 2016 breaks a heat record,” Le Monde announces, the same as almost every year now. All the reasons are there together, but it’s not reasons that make revolutions, it’s bodies. And the bodies are in front of screens.\n",
      "\n",
      "One can watch a presidential election sink like a stone. The transformation of “the most important moment in French political life” into a big trashing fest only makes the soap opera more captivating. One couldn’t imagine Koh-Lanta with such characters, such dizzying plot twists, such cruel tests, or so general a humiliation. The spectacle of politics lives on as the spectacle of its decomposition. Disbelief goes nicely with the filthy landscape. The National Front, that political negation of\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import *\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://theanarchistlibrary.org/library/the-invisible-committe-now.muse'\n",
    "res = requests.get(url)\n",
    "html_page = res.text\n",
    "\n",
    "# Parse the source code using BeautifulSoup\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# Extract the plain text content\n",
    "text = soup.get_text()\n",
    "\n",
    "# Print the plain text\n",
    "print(text[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c57d0783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37621\n",
      "199704\n",
      "3994080\n"
     ]
    }
   ],
   "source": [
    "print(len(kjv))\n",
    "print(len(text))\n",
    "print(len(text * 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f269771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mashup = text * 20 + kjv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "036f027c-6d2f-4d27-a1bb-49ceeb0dfc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37621\n",
      "462682\n",
      "199704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'p'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"fanon.txt\", \"r\")\n",
    "print(len(kjv))\n",
    "print(len((f.read()))) \n",
    "print(len(text))\n",
    "\n",
    "textfile =f.read() + text*2\n",
    "    \n",
    "textfile[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1277f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(textfile, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f2a024dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'existence is then reduced to a collective impotence that mistakes the spectacle of politics : to place politics on the basis of effort always ends up spoiling the cake and smelling of shit . [ellipsis] perceiving exactly what is and provided it lasts for a cortege de tete ,” the more as nothing really is said beyond a movement time frame . taking aim at a riot cop dragging a comrade away for no matter what’s at issue , no longer anything more than satire does . org/files/2018/02/invisible committee now read . then all sorts of incredible actions . “victory through chaos ,” “in ashes , all we have a structure , no government has dared to try and save the particular interests of the customers and real time information and images trying to be broken , and things faced with the world can drive people into the hands of those disgusted with politics . writing , the affect that accompanies it is to head down a dead end street . a planetary generalization of liberal democracy was announced but what characterizes “the collective” as such is precisely that very slight but constant impetus toward tomorrow that is its relentless counterpoint , the end ,” things will change ; “in the absence of work , layoffs or firings , and generally temporary . yellowed figures of a “precariat” conveniently hides the fact of taking a position to go there in the situation . one can rely on the walls had not been socialized ? so that there will surely be solutions tomorrow . despite that , we should make use of their gestures . it never speaks honestly , because wage earning society is defined by its lines of riot police , the radical novelty of the enemy apparatus . one still believes in it , but that last fount of legitimacy has gone dry  '"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646db1d",
   "metadata": {},
   "source": [
    "# Exercise / Homework??\n",
    "\n",
    "Make a mashup of two texts. They can be texts you wrote (a collection of tweets, an essay), or from anywhere. You can use libgen to find books and Calibre to convert them to text. Either paste the text directly into a notebook or use a Python utility for reading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15494939",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = (' ').join(corpiss.gutenberg.words('carroll-alice.txt'))\n",
    "now = text\n",
    "\n",
    "print(len(alice))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1125b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mashups = alice + text\n",
    "model = NgramModel(mashups, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9cef5",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "most used: \n",
    "* https://notebook.community/luketurner/ipython-notebooks/notebooks/n-gram%20tutorial\n",
    "* https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
    "* https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "others:\n",
    "* https://eliteai-coep.medium.com/building-n-gram-language-model-from-scratch-9a5ec206b520\n",
    "* https://github.com/joshualoehr/ngram-language-model/blob/master/language_model.py\n",
    "* http://www.pygaze.org/2016/03/how-to-code-twitter-bot/\n",
    "    - code: https://github.com/esdalmaijer/markovbot\n",
    "* https://towardsdatascience.com/implementing-a-character-level-trigram-language-model-from-scratch-in-python-27ca0e1c3c3f"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
